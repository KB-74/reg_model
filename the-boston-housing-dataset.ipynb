{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6442c27f-a728-4503-966e-8b8b2f5c827f",
    "_uuid": "9a07152bc9041c5d76ffd07cfafe1ac1805d26e1"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1581d9fa-5027-4f68-bd62-5be5c9010f3d",
    "_uuid": "99c61b7d13479ee7d900b1d4f964c49ffd70ebe3"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "collapsed": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "The Boston Housing Dataset\n",
    "\n",
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The following describes the dataset columns:\n",
    "\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per \\$10,000\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "* MEDV - Median value of owner-occupied homes in \\$1000's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f4d62ac-f9eb-457d-9d4d-9683a5a667cc",
    "collapsed": true,
    "_uuid": "7d464c110dc186805b19e709b4443e66d407bdde",
    "trusted": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "d5b0617d-81ab-424f-a1ee-6674925f971e",
    "_uuid": "b08753971c228268b0d2fba0a6978dfcfe9943f4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['housing.csv']\n      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n\n   PTRATIO       B  LSTAT  MEDV  \n0     15.3  396.90   4.98  24.0  \n1     17.8  396.90   9.14  21.6  \n2     17.8  392.83   4.03  34.7  \n3     18.7  394.63   2.94  33.4  \n4     18.7  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Input data files are available in the \"input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "print(os.listdir(\"input\"))\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from pandas import read_csv\n",
    "#Lets load the dataset and sample some\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('input/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fec83140-b3c5-4938-a031-6a34c2ebc45e",
    "_uuid": "79d8bdae5e138554cb4626c784e0cfd43c40606d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "# Dimension of the dataset\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3add881a-52cc-4645-9b2d-d88ecf7779e6",
    "_uuid": "d863cc1813036123eff9879a9387bd980bf0d0ac",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\ncount  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \nmean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \nstd      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \nmin      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \nmax     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n\n              AGE         DIS         RAD         TAX     PTRATIO           B  \\\ncount  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \nmean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \nstd     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \nmin      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \nmax    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n\n            LSTAT        MEDV  \ncount  506.000000  506.000000  \nmean    12.653063   22.532806  \nstd      7.141062    9.197104  \nmin      1.730000    5.000000  \n25%      6.950000   17.025000  \n50%     11.360000   21.200000  \n75%     16.955000   25.000000  \nmax     37.970000   50.000000  \n"
     ]
    }
   ],
   "source": [
    "# Let's summarize the data to see the distribution of data\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b14f79c3-60ce-4d90-b0ec-7e69bd4bb186",
    "_uuid": "94b4ddb51d694c35dbab357788b7e5c4517ecc39"
   },
   "source": [
    "From get-go,  two data coulmns show interesting summeries. They are : ZN (proportion of residential land zoned for lots over 25,000 sq.ft.)  with 0 for 25th, 50th percentiles. Second, CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) with 0 for 25th, 50th and 75th percentiles. These summeries are understandable as both variables are conditional + categorical variables. First assumption would be that these coulms may not be useful in regression task such as predicting MEDV (Median value of owner-occupied homes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5351963b-40ee-4d8b-9729-6a64d35758c3",
    "_uuid": "8ce0ad3eb7208d0488057dc74dc06419fbfc210b"
   },
   "source": [
    "Another interesing fact on the dataset is the max value of MEDV. From the original data description, it says: Variable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000). Based on that, values above 50.00 may not help to predict MEDV. Let's plot the dataset and see interesting trends/stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "23d0dfd5-b7a2-46e4-baed-10ca76a62dbc",
    "_uuid": "50fd4b0697c8c6f9e30c6caa3f60c7d3a03d5a3d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in data.items():\n",
    "    sns.boxplot(y=k, data=data, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e73e8b3a-daf7-49fa-8611-f3ce9a1b2c9c",
    "_uuid": "4d4f0c23bb7761cfb67761df216a8b1bc2e20f75"
   },
   "source": [
    "Columns like CRIM, ZN, RM, B seems to have outliers. Let's see the outliers percentage in every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3db29b4c-9c8c-4457-8064-91d6c3b5ed50",
    "_uuid": "b80e456c7039e0d5c1c3f61e33cb8041ded81622",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column CRIM outliers = 13.04%\nColumn ZN outliers = 13.44%\nColumn INDUS outliers = 0.00%\nColumn CHAS outliers = 100.00%\nColumn NOX outliers = 0.00%\nColumn RM outliers = 5.93%\nColumn AGE outliers = 0.00%\nColumn DIS outliers = 0.99%\nColumn RAD outliers = 0.00%\nColumn TAX outliers = 0.00%\nColumn PTRATIO outliers = 2.96%\nColumn B outliers = 15.22%\nColumn LSTAT outliers = 1.38%\nColumn MEDV outliers = 7.91%\n"
     ]
    }
   ],
   "source": [
    "    for k, v in data.items():\n",
    "        q1 = v.quantile(0.25)\n",
    "        q3 = v.quantile(0.75)\n",
    "        irq = q3 - q1\n",
    "        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
    "        perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n",
    "        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6866af51-50df-4e4a-b268-e29420cb0a99",
    "_uuid": "0251bc88b63788f481a2c05a0ab02acd640a3a8f"
   },
   "source": [
    "Let's remove MEDV outliers (MEDV = 50.0) before plotting more distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "451b476c-6bb5-41b6-abe7-18721c4ea082",
    "_uuid": "4a9d02c119dc56238c7071f1d1d2077708e6b649",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 14)\n"
     ]
    }
   ],
   "source": [
    "data = data[~(data['MEDV'] >= 50.0)]\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4f07967-903d-47ea-9bec-6153e8b18446",
    "_uuid": "d75be26652e9370e490a535db7433f636767a1a8"
   },
   "source": [
    "Let's see how these features plus MEDV distributions looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3365b6f2-14dc-4ec3-9d6b-b5ea48b62971",
    "_uuid": "ba686a43a8c707f42259c3254cb028ff97d0d104",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n/Volumes/Data/jobvink/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    }
   ],
   "source": [
    "# fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "# index = 0\n",
    "# axs = axs.flatten()\n",
    "# for k,v in data.items():\n",
    "#     sns.distplot(v, ax=axs[index])\n",
    "#     index += 1\n",
    "# plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
    "plt.figure(figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k, v in data.items():\n",
    "    sns.distplot(v)\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a7c16fb-3c18-4d76-8ceb-602f43b90aef",
    "_uuid": "a0a4b0a6a28538e9ad4df92da49856f599c25383"
   },
   "source": [
    "The histogram also shows that columns CRIM, ZN, B has highly skewed distributions. Also MEDV looks to have a normal distribution (the predictions) and other colums seem to have norma or bimodel ditribution of data except CHAS (which is a discrete variable).\n",
    "\n",
    "Now let's plot the pairwise  correlation on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_cell_guid": "de1f6ba3-2aab-43ea-ab58-3f938b111ab5",
    "_uuid": "a03fc465f35ebb73358874376569f2fe856c2763",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(data.corr().abs(),  annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "938c8cce-a377-4450-be3d-9d56e2b10f25",
    "_uuid": "6740116517c45740b4c60b2626b6eb477051a52c"
   },
   "source": [
    "From correlation matrix, we see TAX and RAD are highly correlated features. The columns LSTAT, INDUS, RM, TAX, NOX, PTRAIO has a correlation score above 0.5 with MEDV which is a good indication of using as predictors. Let's plot these columns against MEDV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_cell_guid": "8f8a04f5-b4b0-44dc-9034-6b937fe4a530",
    "_uuid": "f03c64c06e33c098efef836457991d49056b7e2a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Let's scale the columns before plotting them against MEDV\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n",
    "x = data.loc[:,column_sels]\n",
    "y = data['MEDV']\n",
    "x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\n",
    "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for i, k in enumerate(column_sels):\n",
    "    sns.regplot(y=y, x=x[k], ax=axs[i])\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54680e1a-addd-4a28-aac1-3065ecf941d2",
    "_uuid": "321b79669416c5e71541539bb10e7c115e78e8ea"
   },
   "source": [
    "So with these analsis, we may try predict MEDV with 'LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE' features. Let's try to remove the skewness of the data trough log transformation.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "trusted": true,
    "_uuid": "8355aaeb269fa9f8cf360d86a01754d43111731e"
   },
   "outputs": [],
   "source": [
    "y =  np.log1p(y)\n",
    "for col in x.columns:\n",
    "    if np.abs(x[col].skew()) > 0.3:\n",
    "        x[col] = np.log1p(x[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f10db586d1b7f15a5eeb441de373210790c41729"
   },
   "source": [
    "Let's try Linear, Ridge Regression on dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_cell_guid": "4c67bb6b-b11e-4da7-906e-93c83ed85c39",
    "_uuid": "4abf281773184265b8e52dad8d58aa72ba41b02a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "l_regression = linear_model.LinearRegression()\n",
    "kf = KFold(n_splits=10)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "scores = cross_val_score(l_regression, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "scores_map = {}\n",
    "scores_map['LinearRegression'] = scores\n",
    "l_ridge = linear_model.Ridge()\n",
    "scores = cross_val_score(l_ridge, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['Ridge'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Lets try polinomial regression with L2 with degree for the best fit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#for degree in range(2, 6):\n",
    "#    model = make_pipeline(PolynomialFeatures(degree=degree), linear_model.Ridge())\n",
    "#    scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "#    print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "model = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\n",
    "scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['PolyRidge'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "241956e9-fa40-4524-b6fc-967d39a807ef",
    "_uuid": "d427ce20b9f9e90a27b324b346fe61cabd8a6753"
   },
   "source": [
    "The Liner Regression with and without L2 regularization does not make significant difference is MSE score. However polynomial regression with degree=3 has a better MSE. Let's try some non prametric regression techniques: SVR with kernal rbf, DecisionTreeRegressor, KNeighborsRegressor etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_kg_hide-input": false,
    "_cell_guid": "c8369e7b-0049-4648-ad23-f5f5d9530cd3",
    "_kg_hide-output": false,
    "_uuid": "3fde7c8a019dcfdfaf60723bde8187923aea2108",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "scores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['SVR'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_cell_guid": "650b9702-13cb-4364-b12f-f996fa013da4",
    "_uuid": "2adbfea5a7d8b2262e71a6f2e9eed2187ce2576b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "desc_tr = DecisionTreeRegressor(max_depth=5)\n",
    "#grid_sv = GridSearchCV(desc_tr, cv=kf, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "scores = cross_val_score(desc_tr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['DecisionTreeRegressor'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_cell_guid": "c0d514db-3056-4a62-a702-979e746073df",
    "_uuid": "ba681bc8dc9405517ed0cfaa2331c1b63b211f73",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=7)\n",
    "scores = cross_val_score(knn, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['KNeighborsRegressor'] = scores\n",
    "#grid_sv = GridSearchCV(knn, cv=kf, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "print(\"KNN Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34daccef-02e5-4b57-8f5b-6039ab0bd39f",
    "_uuid": "d1ee1cbb5abca746bc21a0cccb537d5cf2f94f0a"
   },
   "source": [
    "Compared to three models which are shosen through grid search, SVR performes better. Let's try an ensemble method finally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_cell_guid": "ccee0ea9-9499-4b61-9bbb-f737e7fa2db9",
    "_uuid": "2d6c452ab93d6413688439e100d7368d6019c33c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n",
    "#param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n",
    "#grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "scores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['GradientBoostingRegressor'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd261bb0-fcf4-4b9a-951f-0946b467dbc3",
    "_uuid": "28bc1faf45827a4ec68fc72777e96fa84a065001"
   },
   "source": [
    "Let's plot k-fold results to see which model has better distribution of results. Let's have a look at the MSE distribution of these models with k-fold=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_cell_guid": "8421b80d-d8bd-440f-be5f-0c75cc9a82e6",
    "_uuid": "b65d37fc69ab9b8ced68f0ecc7fe6c11716f39f4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "scores_map = pd.DataFrame(scores_map)\n",
    "sns.boxplot(data=scores_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a16fe973-bbdd-4d4c-a14a-8ab357b9c918",
    "_uuid": "7b6c29011f2e0791ed9239b760eec31fc2f1c37e"
   },
   "source": [
    "The models SVR and GradientBoostingRegressor show better performance with -11.62 (+/- 5.91) and -12.39 (+/- 5.86).\n\nThis is my first kernel and thanks to https://www.kaggle.com/vikrishnan for the dataset and the well writtten kernel that provdies great pointers into this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
